{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for using an LSTM to diagnose ASD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, AveragePooling2D, AveragePooling1D\n",
    "from keras.metrics import BinaryAccuracy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/features')\n",
    "\n",
    "from subject import Subject\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "abide_dir = os.path.dirname(os.path.dirname(cur_dir)) + '/abide/'\n",
    "subjects_dir = os.path.dirname(cur_dir) + '/data/ABIDEI_subjects/'\n",
    "trs_save_file = save_dir = os.path.dirname(cur_dir) + '/data/dicts/ABIDEI_site_trs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with TRs for each scanning site\n",
    "with open(trs_save_file) as json_file:\n",
    "    site_trs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(f):\n",
    "    file = open(f,'rb')\n",
    "    o = pickle.load(file)\n",
    "    file.close()\n",
    "    return o\n",
    "\n",
    "def load_subjects(subject_folder):\n",
    "    subjects = list()\n",
    "    for f in os.listdir(subject_folder):\n",
    "        subjects.append(open_pickle(os.path.join(subject_folder, f)))\n",
    "    return subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = load_subjects(subjects_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now let's just look at sites with Trs of 2s\n",
    "clean_subjects = list()\n",
    "asd_c = 0\n",
    "for s in subjects:\n",
    "    if(site_trs[s._site_id] == 2):\n",
    "        clean_subjects.append(s)\n",
    "        # Note dx group 1 is positive for ASD\n",
    "        if(s._label_dict['dx_group'] == 1):\n",
    "            asd_c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 subjects with ASD out of 548 subjects in clean list\n"
     ]
    }
   ],
   "source": [
    "print(f'{asd_c} subjects with ASD out of {len(clean_subjects)} subjects in clean list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly extract sections of even length from scan to use for features\n",
    "* Doing 3 mins trying to replicate https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5669262/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since scans are 2s apart 90 scans is 3 mins\n",
    "L = 90\n",
    "# Number of clips per subject\n",
    "N=10\n",
    "# Number of ROIs\n",
    "N_rois = 200\n",
    "feat_name = 'filt_noglobal_roi_200_Cradd'\n",
    "def extract_feat_sections(s, feat_name=feat_name, L=L, N=N):\n",
    "    data = s._data_dict[feat_name]\n",
    "    feat_secs = list()\n",
    "    for i in range(N):\n",
    "        r = int(random.random() * (len(data) - L))\n",
    "        feat_secs.append(data[r:r+L])\n",
    "    return np.array(feat_secs)\n",
    "\n",
    "def create_dataset(subjects, feat_name=feat_name, L=L,N=N):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for s in subjects:\n",
    "        feat_secs = extract_feat_sections(s)\n",
    "        X.extend(feat_secs)\n",
    "        # 1 is still classified ASD and 0 is control\n",
    "        if(s._label_dict['dx_group'] == 1):\n",
    "        # if(s._sex == 1):\n",
    "            Y.extend([1]*len(feat_secs))\n",
    "        else:\n",
    "            Y.extend([0]*len(feat_secs))\n",
    "    assert len(X) == len(Y)\n",
    "    X_ar = np.array(X).reshape(len(X), L, N_rois)\n",
    "    Y_ar = np.array(Y)\n",
    "    return X_ar, Y_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465 subjects for training.\n",
      "27 subjects for validation\n",
      "56 subjects for testing\n"
     ]
    }
   ],
   "source": [
    "# In original work 10 fold cross val used with proportion of subjects from each site was approximately the same in all folds\n",
    "# To start will just randomly split subjects into groups\n",
    "val_per = .05\n",
    "test_per = .1\n",
    "train_subs, val_subs = train_test_split(clean_subjects, test_size=val_per + test_per, random_state=42)\n",
    "val_subs, test_subs = train_test_split(val_subs, test_size=test_per/(val_per + test_per), random_state=43)\n",
    "print(f'{len(train_subs)} subjects for training.')\n",
    "print(f'{len(val_subs)} subjects for validation')\n",
    "print(f'{len(test_subs)} subjects for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650 training examples\n",
      "270 validation examples\n",
      "560 testing examples\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = create_dataset(train_subs)\n",
    "val_X, val_Y = create_dataset(val_subs)\n",
    "test_X, test_Y = create_dataset(test_subs)\n",
    "print(f'{len(train_X)} training examples')\n",
    "print(f'{len(val_X)} validation examples')\n",
    "print(f'{len(test_X)} testing examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 90, 16)            13888     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 90, 16)            0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1, 1)              17        \n",
      "=================================================================\n",
      "Total params: 13,905\n",
      "Trainable params: 13,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "# hidden_nodes = int(2/3 * (N_rois * L))\n",
    "hidden_nodes = 16\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_nodes, input_shape=(L, N_rois), return_sequences=True, dropout=0.5))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(LSTM(hidden_nodes))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(AveragePooling2D(pool_size=(L, 1)))\n",
    "model.add(AveragePooling1D(pool_size=L))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "146/146 [==============================] - 23s 160ms/step - loss: 0.5457 - accuracy: 0.7658 - val_loss: 0.6035 - val_accuracy: 0.6407\n",
      "Epoch 2/5\n",
      "146/146 [==============================] - 24s 163ms/step - loss: 0.5286 - accuracy: 0.7740 - val_loss: 0.5988 - val_accuracy: 0.6481\n",
      "Epoch 3/5\n",
      "146/146 [==============================] - 24s 164ms/step - loss: 0.5129 - accuracy: 0.7843 - val_loss: 0.5896 - val_accuracy: 0.6630\n",
      "Epoch 4/5\n",
      "146/146 [==============================] - 25s 171ms/step - loss: 0.4996 - accuracy: 0.7920 - val_loss: 0.6184 - val_accuracy: 0.6148\n",
      "Epoch 5/5\n",
      "146/146 [==============================] - 25s 171ms/step - loss: 0.4813 - accuracy: 0.8049 - val_loss: 0.5427 - val_accuracy: 0.6926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f98d66ddd30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.844, Test: 0.602\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(train_X, train_Y, verbose=0)\n",
    "_, test_acc = model.evaluate(test_X, test_Y, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSingle LSTM layer\\n5 epochs: sex\\nTrain: 0.830, Test: 0.868\\n5 epochs: asd\\nTrain: 0.723, Test: 0.527\\n10 epochs: asd\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Single LSTM layer without pooling\n",
    "5 epochs: sex\n",
    "Train: 0.830, Test: 0.868\n",
    "5 epochs: asd\n",
    "Train: 0.723, Test: 0.527\n",
    "10 epochs: asd\n",
    "'''\n",
    "'''\n",
    "Single LSTM layer without pooling\n",
    "5 epochs: asd\n",
    "Train: 0.675, Test: 0.470\n",
    "10 epochs: asd\n",
    "Train: 0.799, Test: 0.579\n",
    "15 epochs: asd\n",
    "Train: 0.844, Test: 0.602\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_env",
   "language": "python",
   "name": "tf2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
