{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for using an LSTM to diagnose ASD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.metrics import BinaryAccuracy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/features')\n",
    "\n",
    "from subject import Subject\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "abide_dir = os.path.dirname(os.path.dirname(cur_dir)) + '/abide/'\n",
    "subjects_dir = os.path.dirname(cur_dir) + '/data/ABIDEI_subjects/'\n",
    "trs_save_file = save_dir = os.path.dirname(cur_dir) + '/data/dicts/ABIDEI_site_trs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with TRs for each scanning site\n",
    "with open(trs_save_file) as json_file:\n",
    "    site_trs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(f):\n",
    "    file = open(f,'rb')\n",
    "    o = pickle.load(file)\n",
    "    file.close()\n",
    "    return o\n",
    "\n",
    "def load_subjects(subject_folder):\n",
    "    subjects = list()\n",
    "    for f in os.listdir(subject_folder):\n",
    "        subjects.append(open_pickle(os.path.join(subject_folder, f)))\n",
    "    return subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = load_subjects(subjects_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now let's just look at sites with Trs of 2s\n",
    "clean_subjects = list()\n",
    "asd_c = 0\n",
    "for s in subjects:\n",
    "    if(site_trs[s._site_id] == 2):\n",
    "        clean_subjects.append(s)\n",
    "        # Note dx group 1 is positive for ASD\n",
    "        if(s._label_dict['dx_group'] == 1):\n",
    "            asd_c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 subjects with ASD out of 548 subjects in clean list\n"
     ]
    }
   ],
   "source": [
    "print(f'{asd_c} subjects with ASD out of {len(clean_subjects)} subjects in clean list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly extract sections of even length from scan to use for features\n",
    "* Doing 3 mins trying to replicate https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5669262/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since scans are 2s apart 90 scans is 3 mins\n",
    "L = 90\n",
    "# Number of clips per subject\n",
    "N=10\n",
    "# Number of ROIs\n",
    "N_rois = 200\n",
    "feat_name = 'roi_200_Cradd'\n",
    "def extract_feat_sections(s, feat_name=feat_name, L=L, N=N):\n",
    "    data = s._data_dict[feat_name]\n",
    "    feat_secs = list()\n",
    "    for i in range(N):\n",
    "        r = int(random.random() * (len(data) - L))\n",
    "        feat_secs.append(data[r:r+L])\n",
    "    return np.array(feat_secs)\n",
    "\n",
    "def create_dataset(subjects, feat_name=feat_name, L=L,N=N):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for s in subjects:\n",
    "        feat_secs = extract_feat_sections(s)\n",
    "        X.extend(feat_secs)\n",
    "        # 1 is still classified ASD and 0 is control\n",
    "        # if(s._label_dict['dx_group'] == 1):\n",
    "        if(s._sex == 1):\n",
    "            Y.extend([1]*len(feat_secs))\n",
    "        else:\n",
    "            Y.extend([0]*len(feat_secs))\n",
    "    assert len(X) == len(Y)\n",
    "    X_ar = np.array(X).reshape(len(X), L, N_rois)\n",
    "    Y_ar = np.array(Y)\n",
    "    return X_ar, Y_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465 subjects for training\n",
      "27 subjects for validation\n",
      "56 subjects for testing\n"
     ]
    }
   ],
   "source": [
    "# In original work 10 fold cross val used with proportion of subjects from each site was approximately the same in all folds\n",
    "# To start will just randomly split subjects into groups\n",
    "val_per = .05\n",
    "test_per = .1\n",
    "train_subs, val_subs = train_test_split(clean_subjects, test_size=val_per + test_per, random_state=42)\n",
    "val_subs, test_subs = train_test_split(val_subs, test_size=test_per/(val_per + test_per), random_state=43)\n",
    "print(f'{len(train_subs)} subjects for training')\n",
    "print(f'{len(val_subs)} subjects for validation')\n",
    "print(f'{len(test_subs)} subjects for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650 training examples\n",
      "270 validation examples\n",
      "560 testing examples\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = create_dataset(train_subs)\n",
    "val_X, val_Y = create_dataset(val_subs)\n",
    "test_X, test_Y = create_dataset(test_subs)\n",
    "print(f'{len(train_X)} training examples')\n",
    "print(f'{len(val_X)} validation examples')\n",
    "print(f'{len(test_X)} testing examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, 90, 16)            13888     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 90, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 16,017\n",
      "Trainable params: 16,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "# hidden_nodes = int(2/3 * (N_rois * L))\n",
    "hidden_nodes = 16\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_nodes, input_shape=(L, N_rois), return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(hidden_nodes))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "155/155 [==============================] - 7s 44ms/step - loss: 0.5206 - accuracy: 0.7875 - val_loss: 0.2654 - val_accuracy: 0.9630\n",
      "Epoch 2/50\n",
      "155/155 [==============================] - 4s 29ms/step - loss: 0.4342 - accuracy: 0.8172 - val_loss: 0.2278 - val_accuracy: 0.9519\n",
      "Epoch 3/50\n",
      "155/155 [==============================] - 4s 29ms/step - loss: 0.3445 - accuracy: 0.8551 - val_loss: 0.2088 - val_accuracy: 0.9148\n",
      "Epoch 4/50\n",
      "155/155 [==============================] - 5s 30ms/step - loss: 0.3316 - accuracy: 0.8718 - val_loss: 0.2636 - val_accuracy: 0.8630\n",
      "Epoch 5/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.2363 - accuracy: 0.9151 - val_loss: 0.2401 - val_accuracy: 0.9148\n",
      "Epoch 6/50\n",
      "155/155 [==============================] - 5s 31ms/step - loss: 0.1878 - accuracy: 0.9372 - val_loss: 0.3846 - val_accuracy: 0.8148\n",
      "Epoch 7/50\n",
      "155/155 [==============================] - 5s 29ms/step - loss: 0.1631 - accuracy: 0.9452 - val_loss: 0.2656 - val_accuracy: 0.9111\n",
      "Epoch 8/50\n",
      "155/155 [==============================] - 5s 32ms/step - loss: 0.1286 - accuracy: 0.9572 - val_loss: 0.2486 - val_accuracy: 0.9296\n",
      "Epoch 9/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.1234 - accuracy: 0.9566 - val_loss: 0.3541 - val_accuracy: 0.8444\n",
      "Epoch 10/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.1081 - accuracy: 0.9611 - val_loss: 0.3691 - val_accuracy: 0.8778\n",
      "Epoch 11/50\n",
      "155/155 [==============================] - 5s 35ms/step - loss: 0.1003 - accuracy: 0.9654 - val_loss: 0.3188 - val_accuracy: 0.9111\n",
      "Epoch 12/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.0905 - accuracy: 0.9680 - val_loss: 0.2712 - val_accuracy: 0.9407\n",
      "Epoch 13/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.0621 - accuracy: 0.9798 - val_loss: 0.4535 - val_accuracy: 0.8556\n",
      "Epoch 14/50\n",
      "155/155 [==============================] - 5s 32ms/step - loss: 0.0799 - accuracy: 0.9699 - val_loss: 0.6332 - val_accuracy: 0.8148\n",
      "Epoch 15/50\n",
      "155/155 [==============================] - 5s 32ms/step - loss: 0.0497 - accuracy: 0.9834 - val_loss: 0.3788 - val_accuracy: 0.9074\n",
      "Epoch 16/50\n",
      "155/155 [==============================] - 5s 32ms/step - loss: 0.0552 - accuracy: 0.9815 - val_loss: 0.3964 - val_accuracy: 0.9185\n",
      "Epoch 17/50\n",
      "155/155 [==============================] - 5s 34ms/step - loss: 0.0415 - accuracy: 0.9875 - val_loss: 0.5142 - val_accuracy: 0.8852\n",
      "Epoch 18/50\n",
      "155/155 [==============================] - 5s 34ms/step - loss: 0.0426 - accuracy: 0.9871 - val_loss: 0.2907 - val_accuracy: 0.9481\n",
      "Epoch 19/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.0493 - accuracy: 0.9832 - val_loss: 0.2405 - val_accuracy: 0.9630\n",
      "Epoch 20/50\n",
      "155/155 [==============================] - 5s 32ms/step - loss: 0.0430 - accuracy: 0.9877 - val_loss: 0.4313 - val_accuracy: 0.8963\n",
      "Epoch 21/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.0451 - accuracy: 0.9862 - val_loss: 0.4298 - val_accuracy: 0.9074\n",
      "Epoch 22/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.0320 - accuracy: 0.9890 - val_loss: 0.3921 - val_accuracy: 0.9148\n",
      "Epoch 23/50\n",
      "155/155 [==============================] - 5s 33ms/step - loss: 0.0438 - accuracy: 0.9845 - val_loss: 0.3916 - val_accuracy: 0.9296\n",
      "Epoch 24/50\n",
      "155/155 [==============================] - 6s 36ms/step - loss: 0.0600 - accuracy: 0.9798 - val_loss: 0.4625 - val_accuracy: 0.8963\n",
      "Epoch 25/50\n",
      "155/155 [==============================] - 6s 39ms/step - loss: 0.0676 - accuracy: 0.9778 - val_loss: 0.5455 - val_accuracy: 0.8556\n",
      "Epoch 26/50\n",
      "155/155 [==============================] - 6s 37ms/step - loss: 0.0481 - accuracy: 0.9837 - val_loss: 0.4867 - val_accuracy: 0.8815\n",
      "Epoch 27/50\n",
      "155/155 [==============================] - 6s 37ms/step - loss: 0.0350 - accuracy: 0.9871 - val_loss: 0.6305 - val_accuracy: 0.8481\n",
      "Epoch 28/50\n",
      "155/155 [==============================] - 6s 37ms/step - loss: 0.0355 - accuracy: 0.9895 - val_loss: 0.3535 - val_accuracy: 0.9074\n",
      "Epoch 29/50\n",
      "155/155 [==============================] - 6s 36ms/step - loss: 0.0382 - accuracy: 0.9875 - val_loss: 0.3281 - val_accuracy: 0.9296\n",
      "Epoch 30/50\n",
      "155/155 [==============================] - 5s 34ms/step - loss: 0.0268 - accuracy: 0.9910 - val_loss: 0.3460 - val_accuracy: 0.9444\n",
      "Epoch 31/50\n",
      "155/155 [==============================] - 6s 38ms/step - loss: 0.0202 - accuracy: 0.9944 - val_loss: 0.3143 - val_accuracy: 0.9481\n",
      "Epoch 32/50\n",
      "155/155 [==============================] - 6s 36ms/step - loss: 0.0294 - accuracy: 0.9903 - val_loss: 0.3426 - val_accuracy: 0.8963\n",
      "Epoch 33/50\n",
      "155/155 [==============================] - 5s 34ms/step - loss: 0.0379 - accuracy: 0.9877 - val_loss: 0.5477 - val_accuracy: 0.8778\n",
      "Epoch 34/50\n",
      "155/155 [==============================] - 6s 37ms/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.3346 - val_accuracy: 0.9111\n",
      "Epoch 35/50\n",
      "155/155 [==============================] - 6s 38ms/step - loss: 0.0418 - accuracy: 0.9858 - val_loss: 0.5188 - val_accuracy: 0.8444\n",
      "Epoch 36/50\n",
      "155/155 [==============================] - 6s 38ms/step - loss: 0.0351 - accuracy: 0.9880 - val_loss: 0.2953 - val_accuracy: 0.9481\n",
      "Epoch 37/50\n",
      "155/155 [==============================] - 6s 37ms/step - loss: 0.0289 - accuracy: 0.9910 - val_loss: 0.4850 - val_accuracy: 0.8963\n",
      "Epoch 38/50\n",
      "155/155 [==============================] - 6s 37ms/step - loss: 0.0289 - accuracy: 0.9903 - val_loss: 0.2953 - val_accuracy: 0.9407\n",
      "Epoch 39/50\n",
      "155/155 [==============================] - 7s 42ms/step - loss: 0.0349 - accuracy: 0.9869 - val_loss: 0.2603 - val_accuracy: 0.9444\n",
      "Epoch 40/50\n",
      "155/155 [==============================] - 6s 42ms/step - loss: 0.0245 - accuracy: 0.9927 - val_loss: 0.1409 - val_accuracy: 0.9593\n",
      "Epoch 41/50\n",
      "155/155 [==============================] - 6s 37ms/step - loss: 0.0313 - accuracy: 0.9903 - val_loss: 0.2997 - val_accuracy: 0.9296\n",
      "Epoch 42/50\n",
      "155/155 [==============================] - 5s 34ms/step - loss: 0.0358 - accuracy: 0.9873 - val_loss: 0.2375 - val_accuracy: 0.9370\n",
      "Epoch 43/50\n",
      "155/155 [==============================] - 6s 38ms/step - loss: 0.0354 - accuracy: 0.9871 - val_loss: 0.2215 - val_accuracy: 0.9222\n",
      "Epoch 44/50\n",
      "155/155 [==============================] - 6s 36ms/step - loss: 0.0210 - accuracy: 0.9920 - val_loss: 0.3867 - val_accuracy: 0.8963\n",
      "Epoch 45/50\n",
      "155/155 [==============================] - 6s 40ms/step - loss: 0.0201 - accuracy: 0.9925 - val_loss: 0.5990 - val_accuracy: 0.8593\n",
      "Epoch 46/50\n",
      "155/155 [==============================] - 6s 36ms/step - loss: 0.0308 - accuracy: 0.9903 - val_loss: 0.6677 - val_accuracy: 0.8444\n",
      "Epoch 47/50\n",
      "155/155 [==============================] - 5s 35ms/step - loss: 0.0199 - accuracy: 0.9920 - val_loss: 0.3985 - val_accuracy: 0.9185\n",
      "Epoch 48/50\n",
      "155/155 [==============================] - 5s 34ms/step - loss: 0.0212 - accuracy: 0.9918 - val_loss: 0.6006 - val_accuracy: 0.8889\n",
      "Epoch 49/50\n",
      "155/155 [==============================] - 5s 34ms/step - loss: 0.0314 - accuracy: 0.9895 - val_loss: 0.3543 - val_accuracy: 0.9370\n",
      "Epoch 50/50\n",
      "155/155 [==============================] - 5s 34ms/step - loss: 0.0368 - accuracy: 0.9867 - val_loss: 0.4258 - val_accuracy: 0.8926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d95f187e50>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "epochs = 50\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.997, Test: 0.832\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(train_X, train_Y, verbose=0)\n",
    "_, test_acc = model.evaluate(test_X, test_Y, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tf2",
   "language": "python",
   "name": "tf2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
