{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "circular-preparation",
   "metadata": {},
   "source": [
    "# Notebook for creating data for model training on Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flexible-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/features')\n",
    "\n",
    "from subject import Subject\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "abide_dir = os.path.dirname(os.path.dirname(cur_dir)) + '/abide/'\n",
    "data_dir = os.path.dirname(cur_dir) +'/data/'\n",
    "ab_subjects_dir = os.path.dirname(cur_dir) + '/data/ABIDEI_subjects/'\n",
    "ab2_subjects_dir = os.path.dirname(cur_dir) + '/data/ABIDEII_subjects/'\n",
    "trs_save_file = save_dir = os.path.dirname(cur_dir) + '/data/dicts/ABIDEI_site_trs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "asian-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with TRs for each scanning site\n",
    "with open(trs_save_file) as json_file:\n",
    "    site_trs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elegant-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ABIDEI preprocessed rois by loading subjects\n",
    "def open_pickle(f):\n",
    "    file = open(f,'rb')\n",
    "    o = pickle.load(file)\n",
    "    file.close()\n",
    "    return o\n",
    "\n",
    "def load_subjects_d(subject_folder):\n",
    "    subjects_d = {}\n",
    "    for f in os.listdir(subject_folder):\n",
    "        s = open_pickle(os.path.join(subject_folder, f))\n",
    "        subjects_d[s._sub_id] = s\n",
    "    return subjects_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demographic-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_subjects(subjects, trs=[2, 2.17]):\n",
    "    clean_s = list()\n",
    "    trs = set(trs)\n",
    "    for s in subjects:\n",
    "        if(s._tr in trs):\n",
    "            clean_s.append(s)\n",
    "    print(f'{len(clean_s)} clean out of {len(subjects)}')\n",
    "    return clean_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "painted-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548 clean out of 884\n"
     ]
    }
   ],
   "source": [
    "ab_subjects_d = load_subjects_d(ab_subjects_dir)\n",
    "ab_subjects = list(ab_subjects_d.values())\n",
    "ab_subjects_c = clean_subjects(ab_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flush-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 clean out of 458\n"
     ]
    }
   ],
   "source": [
    "ab2_subjects_d = load_subjects_d(ab2_subjects_dir)\n",
    "ab2_subjects = list(ab2_subjects_d.values())\n",
    "ab2_subjects_c = clean_subjects(ab2_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "induced-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 clean out of 161\n"
     ]
    }
   ],
   "source": [
    "adhd_subjects_dir = os.path.dirname(cur_dir) + '/data/ADHD200_subjects/'\n",
    "adhd_subjects_d = load_subjects_d(adhd_subjects_dir)\n",
    "adhd_subjects = list(adhd_subjects_d.values())\n",
    "adhd_subjects_c = clean_subjects(adhd_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "precise-healthcare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 clean out of 155\n"
     ]
    }
   ],
   "source": [
    "acpi_subjects_dir = os.path.dirname(cur_dir) + '/data/ACPI_subjects/'\n",
    "acpi_subjects_d = load_subjects_d(acpi_subjects_dir)\n",
    "acpi_subjects = list(acpi_subjects_d.values())\n",
    "acpi_subjects_c = clean_subjects(acpi_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "painted-settlement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186\n"
     ]
    }
   ],
   "source": [
    "ab_subjects_c.extend(ab2_subjects_c)\n",
    "ab_subjects_c.extend(adhd_subjects_c)\n",
    "ab_subjects_c.extend(acpi_subjects_c)\n",
    "clean_subjects = ab_subjects_c\n",
    "print(len(clean_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bulgarian-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now let's just look at sites with Trs of 2s\n",
    "# clean_subjects = list()\n",
    "# asd_c = 0\n",
    "# for s in subjects:\n",
    "#     if(s._tr == 2):\n",
    "#         clean_subjects.append(s)\n",
    "#         # Note dx group 1 is positive for ASD\n",
    "#         if(s._label_dict['dx_group'] == 1):\n",
    "#             asd_c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hollow-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 subjects with ASD out of 949 subjects in clean list\n"
     ]
    }
   ],
   "source": [
    "print(f'{asd_c} subjects with ASD out of {len(clean_subjects)} subjects in clean list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-benjamin",
   "metadata": {},
   "source": [
    "# Create Training Data files for Colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latter-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_name = 'filt_noglobal_roi_200_Cradd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "regulation-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()\n",
    "Y = list()\n",
    "s_c = 0\n",
    "for s in clean_subjects:\n",
    "    X.append(np.array(s._data_dict[feat_name]))\n",
    "    asd = s._label_dict['dx_group'] == 1\n",
    "    # 1 is male\n",
    "    sex = s._sex == 1\n",
    "    s_c += sex\n",
    "    age = s._age\n",
    "    Y.append(np.array([sex, age, asd]))\n",
    "assert len(X) == len(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acting-generator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "elect-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-e9d2dad0393c>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X = np.array(X)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "endless-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = data_dir + 'colab_training/'\n",
    "np.save(training_dir + 'total_X', X)\n",
    "np.save(training_dir + 'total_Y', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-batch",
   "metadata": {},
   "source": [
    "# Randomly extract sections of even length from scan to use for features\n",
    "* Doing 3 mins trying to replicate https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5669262/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eastern-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since scans are 2s apart 90 scans is 3 mins\n",
    "L = 90\n",
    "# Number of clips per subject\n",
    "N=10\n",
    "# Number of ROIs\n",
    "N_rois = 200\n",
    "feat_name = 'filt_noglobal_roi_200_Cradd'\n",
    "def extract_feat_sections(s, feat_name=feat_name, L=L, N=N):\n",
    "    data = s._data_dict[feat_name]\n",
    "    feat_secs = list()\n",
    "    for i in range(N):\n",
    "        r = int(random.random() * (len(data) - L))\n",
    "        feat_secs.append(data[r:r+L])\n",
    "    return np.array(feat_secs)\n",
    "\n",
    "def create_dataset(subjects, feat_name=feat_name, L=L,N=N):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for s in subjects:\n",
    "        feat_secs = extract_feat_sections(s)\n",
    "        X.extend(feat_secs)\n",
    "        # 1 is still classified ASD and 0 is control\n",
    "        asd = s._label_dict['dx_group'] == 1\n",
    "        sex = s._sex == 1\n",
    "        for i in range(len(feat_secs)):\n",
    "            Y.append(np.array([asd, sex]))\n",
    "#         # if(s._sex == 1):\n",
    "#             Y.extend([1]*len(feat_secs))\n",
    "#         else:\n",
    "#             Y.extend([0]*len(feat_secs))\n",
    "    assert len(X) == len(Y)\n",
    "    X_ar = np.array(X).reshape(len(X), L, N_rois)\n",
    "    # X_ar = np.array(X)\n",
    "    Y_ar = np.array(Y)\n",
    "    return X_ar, Y_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fifteen-proposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465 subjects for training\n",
      "27 subjects for validation\n",
      "56 subjects for testing\n"
     ]
    }
   ],
   "source": [
    "val_per = .05\n",
    "test_per = .1\n",
    "train_subs, val_subs = train_test_split(clean_subjects, test_size=val_per + test_per, random_state=42)\n",
    "val_subs, test_subs = train_test_split(val_subs, test_size=test_per/(val_per + test_per), random_state=43)\n",
    "print(f'{len(train_subs)} subjects for training')\n",
    "print(f'{len(val_subs)} subjects for validation')\n",
    "print(f'{len(test_subs)} subjects for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "closed-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = create_dataset(train_subs)\n",
    "val_X, val_Y = create_dataset(val_subs)\n",
    "test_X, test_Y = create_dataset(test_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "indian-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = data_dir + 'colab_training/'\n",
    "np.save(training_dir + 'train_X', train_X)\n",
    "np.save(training_dir + 'train_Y', train_Y)\n",
    "np.save(training_dir + 'val_X', val_X)\n",
    "np.save(training_dir + 'val_Y', val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-carroll",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_env",
   "language": "python",
   "name": "tf2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
