{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sorted-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/features')\n",
    "\n",
    "from subject import Subject\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "abide_dir = os.path.dirname(os.path.dirname(cur_dir)) + '/abide/'\n",
    "subjects_dir = os.path.dirname(cur_dir) + '/data/ABIDEI_subjects/'\n",
    "trs_save_file = save_dir = os.path.dirname(cur_dir) + '/data/dicts/ABIDEI_site_trs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "considered-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with TRs for each scanning site\n",
    "with open(trs_save_file) as json_file:\n",
    "    site_trs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "funded-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ABIDEI preprocessed rois by loading subjects\n",
    "def open_pickle(f):\n",
    "    file = open(f,'rb')\n",
    "    o = pickle.load(file)\n",
    "    file.close()\n",
    "    return o\n",
    "\n",
    "def load_subjects_d(subject_folder):\n",
    "    subjects_d = {}\n",
    "    for f in os.listdir(subject_folder):\n",
    "        s = open_pickle(os.path.join(subject_folder, f))\n",
    "        subjects_d[s._sub_id] = s\n",
    "    return subjects_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "artificial-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_d = load_subjects_d(subjects_dir)\n",
    "subjects = subjects_d.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dutch-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now let's just look at sites with Trs of 2s\n",
    "clean_subjects = list()\n",
    "asd_c = 0\n",
    "for s in subjects:\n",
    "    if(site_trs[s._site_id] == 2):\n",
    "        clean_subjects.append(s)\n",
    "        # Note dx group 1 is positive for ASD\n",
    "        if(s._label_dict['dx_group'] == 1):\n",
    "            asd_c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "casual-sucking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 subjects with ASD out of 548 subjects in clean list\n"
     ]
    }
   ],
   "source": [
    "print(f'{asd_c} subjects with ASD out of {len(clean_subjects)} subjects in clean list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-classification",
   "metadata": {},
   "source": [
    "# Randomly extract sections of even length from scan to use for features\n",
    "* Doing 3 mins trying to replicate https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5669262/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protective-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since scans are 2s apart 90 scans is 3 mins\n",
    "L = 90\n",
    "# Number of clips per subject\n",
    "N=10\n",
    "# Number of ROIs\n",
    "N_rois = 200\n",
    "feat_name = 'filt_noglobal_roi_200_Cradd'\n",
    "def extract_feat_sections(s, feat_name=feat_name, L=L, N=N):\n",
    "    data = s._data_dict[feat_name]\n",
    "    feat_secs = list()\n",
    "    for i in range(N):\n",
    "        r = int(random.random() * (len(data) - L))\n",
    "        feat_secs.append(data[r:r+L])\n",
    "    return np.array(feat_secs)\n",
    "\n",
    "def create_dataset(subjects, feat_name=feat_name, L=L,N=N):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for s in subjects:\n",
    "        feat_secs = extract_feat_sections(s)\n",
    "        X.extend(feat_secs)\n",
    "        # 1 is still classified ASD and 0 is control\n",
    "        if(s._label_dict['dx_group'] == 1):\n",
    "        # if(s._sex == 1):\n",
    "            Y.extend([1]*len(feat_secs))\n",
    "        else:\n",
    "            Y.extend([0]*len(feat_secs))\n",
    "    assert len(X) == len(Y)\n",
    "    X_ar = np.array(X).reshape(len(X), L, N_rois)\n",
    "    Y_ar = np.array(Y)\n",
    "    return X_ar, Y_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "printable-health",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465 subjects for training\n",
      "27 subjects for validation\n",
      "56 subjects for testing\n"
     ]
    }
   ],
   "source": [
    "# In original work 10 fold cross val used with proportion of subjects from each site was approximately the same in all folds\n",
    "# To start will just randomly split subjects into groups\n",
    "val_per = .05\n",
    "test_per = .1\n",
    "train_subs, val_subs = train_test_split(clean_subjects, test_size=val_per + test_per, random_state=42)\n",
    "val_subs, test_subs = train_test_split(val_subs, test_size=test_per/(val_per + test_per), random_state=43)\n",
    "print(f'{len(train_subs)} subjects for training')\n",
    "print(f'{len(val_subs)} subjects for validation')\n",
    "print(f'{len(test_subs)} subjects for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thousand-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650 training examples. 2110 of class 1\n",
      "270 validation examples. 110 of class 1\n",
      "560 testing examples. 310 of class 1\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = create_dataset(train_subs)\n",
    "val_X, val_Y = create_dataset(val_subs)\n",
    "test_X, test_Y = create_dataset(test_subs)\n",
    "print(f'{len(train_X)} training examples. {sum(train_Y)} of class 1')\n",
    "print(f'{len(val_X)} validation examples. {sum(val_Y)} of class 1')\n",
    "print(f'{len(test_X)} testing examples. {sum(test_Y)} of class 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-debate",
   "metadata": {},
   "source": [
    "# Create Transformer model\n",
    "* Resource: https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "vanilla-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "worthy-blade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 90, 200)]         0         \n",
      "_________________________________________________________________\n",
      "transformer_block_3 (Transfo (None, 90, 200)           335232    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 90, 200)           0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 90, 20)            4020      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 90, 20)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 90, 1)             21        \n",
      "=================================================================\n",
      "Total params: 339,273\n",
      "Trainable params: 339,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = N_rois  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "max_len = L\n",
    "\n",
    "# inputs = layers.Input(shape=(max_len,))\n",
    "inputs = layers.Input(shape=(max_len, embed_dim))\n",
    "# embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "# x = embedding_layer(inputs)\n",
    "x = inputs\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "# x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "blank-assist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "146/146 [==============================] - 58s 383ms/step - loss: 0.7313 - accuracy: 0.5311 - val_loss: 0.6872 - val_accuracy: 0.5660\n",
      "Epoch 2/5\n",
      "146/146 [==============================] - 59s 400ms/step - loss: 0.6721 - accuracy: 0.5656 - val_loss: 0.6881 - val_accuracy: 0.5629\n",
      "Epoch 3/5\n",
      "146/146 [==============================] - 56s 383ms/step - loss: 0.6293 - accuracy: 0.6427 - val_loss: 0.7296 - val_accuracy: 0.5345\n",
      "Epoch 4/5\n",
      "146/146 [==============================] - 57s 390ms/step - loss: 0.5776 - accuracy: 0.6938 - val_loss: 0.7935 - val_accuracy: 0.5581\n",
      "Epoch 5/5\n",
      "146/146 [==============================] - 59s 401ms/step - loss: 0.5164 - accuracy: 0.7450 - val_loss: 0.9263 - val_accuracy: 0.5146\n"
     ]
    }
   ],
   "source": [
    "# model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    train_X, train_Y, batch_size=32, epochs=5, validation_data=(val_X, val_Y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "polyphonic-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.717, Test: 0.505\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(train_X, train_Y, verbose=0)\n",
    "_, test_acc = model.evaluate(test_X, test_Y, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-festival",
   "metadata": {},
   "source": [
    "# Now with a positional embedding layer\n",
    "* https://keras.io/examples/nlp/masked_language_modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "requested-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "thick-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 90, 200)]         0         \n",
      "_________________________________________________________________\n",
      "position_embedding_3 (Positi (None, 90, 200)           18000     \n",
      "_________________________________________________________________\n",
      "transformer_block_6 (Transfo (None, 90, 200)           335232    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 90, 200)           0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 90, 20)            4020      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 90, 20)            0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 90, 1)             21        \n",
      "=================================================================\n",
      "Total params: 357,273\n",
      "Trainable params: 357,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = N_rois  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "max_len = L\n",
    "\n",
    "# inputs = layers.Input(shape=(max_len,))\n",
    "inputs = layers.Input(shape=(max_len, embed_dim))\n",
    "pos_embedding_layer = PositionEmbedding(max_len, embed_dim)\n",
    "x = pos_embedding_layer(inputs)\n",
    "# x = inputs\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "# x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model_embed = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_embed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "associate-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "146/146 [==============================] - 71s 459ms/step - loss: 0.7204 - accuracy: 0.5329 - val_loss: 0.6699 - val_accuracy: 0.5822\n",
      "Epoch 2/2\n",
      "146/146 [==============================] - 69s 470ms/step - loss: 0.6542 - accuracy: 0.6131 - val_loss: 0.6731 - val_accuracy: 0.5786\n"
     ]
    }
   ],
   "source": [
    "model_embed.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model_embed.fit(\n",
    "    train_X, train_Y, batch_size=32, epochs=2, validation_data=(val_X, val_Y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model_embed.evaluate(train_X, train_Y, verbose=0)\n",
    "_, test_acc = model_embed.evaluate(test_X, test_Y, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-august",
   "metadata": {},
   "source": [
    "# Transformer pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-length",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_env",
   "language": "python",
   "name": "tf2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
